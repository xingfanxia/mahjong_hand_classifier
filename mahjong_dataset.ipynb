{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJMCMHwFcC1I"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kGWIFVVPaMx3"
      },
      "outputs": [],
      "source": [
        "from glob import glob \n",
        "# from google.colab import files\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import numpy as np\n",
        "import cv2\n",
        "import progressbar\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz-TXokVshuI"
      },
      "source": [
        "**Retrieve images from external sources**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xxnba9tyEDT",
        "outputId": "b12b92e7-82be-470e-be27-45fce4ee5aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mahjong-dataset' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone the GitHub repo of mahjong tile images\n",
        "!git clone https://github.com/camerash/mahjong-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqj--Gy6g0YQ",
        "outputId": "2b3816eb-e4e0-4566-f618-af702504d2bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-11 22:44:08--  https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/datasets/dtd/dtd-r1.0.1.tar.gz [following]\n",
            "--2024-03-11 22:44:09--  https://thor.robots.ox.ac.uk/datasets/dtd/dtd-r1.0.1.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 625239812 (596M) [application/octet-stream]\n",
            "Saving to: ‘dtd-r1.0.1.tar.gz’\n",
            "\n",
            "dtd-r1.0.1.tar.gz   100%[===================>] 596.27M  24.3MB/s    in 26s     \n",
            "\n",
            "2024-03-11 22:44:35 (23.3 MB/s) - ‘dtd-r1.0.1.tar.gz’ saved [625239812/625239812]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Describable Textures Dataset (DTD)\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fzkm0eIpg0yW"
      },
      "outputs": [],
      "source": [
        "# Extract the DTD\n",
        "!tar xf dtd-r1.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fdZLUYX-g154"
      },
      "outputs": [],
      "source": [
        "# Delete the zip folder of DTD\n",
        "!rm dtd-r1.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-K4nMUBg49U",
        "outputId": "5e0ee940-afec-4dc0-9a05-ffa4f94d987b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading images ... (It could take several minutes)\n",
            "Number of images loaded : 5640\n"
          ]
        }
      ],
      "source": [
        "# Load all *jpg from dtd subdirectories and save them in a pickle file (1x)\n",
        "\n",
        "backgrounds_pck_fn=\"backgrounds.pck\"\n",
        "dtd_dir=\"dtd/images/\"\n",
        "bg_images=[]\n",
        "print(\"Loading images ... (It could take several minutes)\")\n",
        "for subdir in glob(dtd_dir+\"/*\"):\n",
        "    for f in glob(subdir+\"/*.jpg\"):\n",
        "        bg_images.append(mpimg.imread(f))\n",
        "pickle.dump(bg_images,open(backgrounds_pck_fn,'wb'))\n",
        "\n",
        "class Backgrounds():\n",
        "    def __init__(self,backgrounds_pck_fn=backgrounds_pck_fn):\n",
        "        self._images=pickle.load(open(backgrounds_pck_fn,'rb'))\n",
        "        self._nb_images=len(self._images)\n",
        "        print(\"Number of images loaded :\", self._nb_images)\n",
        "    def get_random(self, display=False):\n",
        "        bg=self._images[random.randint(0,self._nb_images-1)]\n",
        "        if display: plt.imshow(bg)\n",
        "        return bg\n",
        "\n",
        "backgrounds = Backgrounds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrmhNjM4dQvt"
      },
      "source": [
        "**Edit directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LhOzuvRR5Stx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: images: File exists\n"
          ]
        }
      ],
      "source": [
        "# Create a directory that will contain the images we generate\n",
        "data_dir=\"images\"\n",
        "!mkdir images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJBBYGpbdTKF"
      },
      "source": [
        "**Define functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CskbcMZG6M9L"
      },
      "outputs": [],
      "source": [
        "# Rotate the image to the angle as specified\n",
        "def rotate_tile(image, angle):\n",
        "    # grab the dimensions of the image and then determine the\n",
        "    # center\n",
        "    (h, w) = image.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    border_color=(255, 255, 255)\n",
        "\n",
        "    # grab the rotation matrix (applying the negative of the\n",
        "    # angle to rotate clockwise), then grab the sine and cosine\n",
        "    # (i.e. the rotation components of the matrix)\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        " \n",
        "    # compute the new bounding dimensions of the image\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        " \n",
        "    # adjust the rotation matrix to take into account translation\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        " \n",
        "    # perform the actual rotation and return the image\n",
        "    return cv2.warpAffine(image, M, (nW, nH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mkuReCFD_7CT"
      },
      "outputs": [],
      "source": [
        "# Resize an image\n",
        "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
        "    # initialize the dimensions of the image to be resized and\n",
        "    # grab the image size\n",
        "    dim = None\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    # if both the width and height are None, then return the\n",
        "    # original image\n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "\n",
        "    # check to see if the width is None\n",
        "    if width is None:\n",
        "        # calculate the ratio of the height and construct the\n",
        "        # dimensions\n",
        "        r = height / float(h)\n",
        "        dim = (int(w * r), height)\n",
        "\n",
        "    # otherwise, the height is None\n",
        "    else:\n",
        "        # calculate the ratio of the width and construct the\n",
        "        # dimensions\n",
        "        r = width / float(w)\n",
        "        dim = (width, int(h * r))\n",
        "\n",
        "    # resize the image\n",
        "    resized = cv2.resize(image, dim, interpolation = inter)\n",
        "\n",
        "    # return the resized image\n",
        "    return resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vCe_1VC9pyFm"
      },
      "outputs": [],
      "source": [
        "def create_image_in_grid_format(csv_filename, grid_length=4):\n",
        "  (tile_images, tile_type_of_tile_images) = genrate_images(grid_length)\n",
        "  tile_type_of_tile_images_for_output = tile_type_of_tile_images[:]\n",
        "\n",
        "  # Randomly select a background image\n",
        "  background_image = backgrounds.get_random()\n",
        "\n",
        "  # Copy the background image such that the image would not be mutated  \n",
        "  output_image = cv2.cvtColor(background_image, cv2.COLOR_RGB2RGBA).copy()\n",
        "  \n",
        "  # Resize the background image to standard width and height (i.e. 1024x1024)\n",
        "  # The resize action would result int distortion\n",
        "  resize_specifications = (image_standard_width, image_standard_height)\n",
        "  output_image = cv2.resize(output_image, resize_specifications, \n",
        "                            interpolation = cv2.INTER_AREA)\n",
        "  csv_row_data_placeholder = []\n",
        "\n",
        "  # Place the grid on the resized background image, \n",
        "  # where the black area of the grid would be perceived as transparent\n",
        "  for row_index in range(grid_length): \n",
        "    for column_index in range(grid_length):\n",
        "      tile_img = tile_images[0]\n",
        "      tmp = cv2.cvtColor(tile_img, cv2.COLOR_BGR2GRAY)\n",
        "      _,alpha = cv2.threshold(tmp,0,255,cv2.THRESH_BINARY)\n",
        "      b, g, r, a = cv2.split(tile_img)\n",
        "      rgba = [b,g,r, alpha]\n",
        "      tile_img = cv2.merge(rgba,4)\n",
        "\n",
        "      x_offset = int(((column_index / grid_length) * output_image.shape[1]))\n",
        "      y_offset = int((((row_index) / grid_length) * output_image.shape[0]))\n",
        "\n",
        "      y1, y2 = y_offset, y_offset + tile_img.shape[0]\n",
        "      x1, x2 = x_offset, x_offset + tile_img.shape[1]\n",
        "      \n",
        "      alpha_s = tile_img[:, :, 3] / 255.0\n",
        "      alpha_l = 1.0 - alpha_s\n",
        "      \n",
        "      for c in range(0, 3):\n",
        "        output_image[y1:y2, x1:x2, c] = (alpha_s * tile_img[:, :, c] + alpha_l \n",
        "                                         * output_image[y1:y2, x1:x2, c])\n",
        "        \n",
        "      # Compute the value of relative coordinates\n",
        "      min_x = x1/output_image.shape[1]\n",
        "      max_x = x2/output_image.shape[1]\n",
        "      min_y = y1/output_image.shape[0]\n",
        "      max_y = y2/output_image.shape[0]\n",
        "\n",
        "      # Write data to csv: \n",
        "      # (1) the image filename, \n",
        "      # (2) tile type and \n",
        "      # (3) top-left and bottom-right vertices coordinates \n",
        "      #     of the mahjong tile in the image \n",
        "      csv_row_data_placeholder.append(\n",
        "          (tile_type_of_tile_images[0],min_x,min_y,max_x,max_y))\n",
        "      \n",
        "      del tile_images[0]\n",
        "      del tile_type_of_tile_images[0]\n",
        "\n",
        "  # Save the image\n",
        "  now = datetime.datetime.now()\n",
        "  \n",
        "  string_of_current_moment = str(now.year) + '-' + \\\n",
        "                              str(now.month).zfill(2) + '-' + \\\n",
        "                              str(now.day).zfill(2) + '-'  + \\\n",
        "                              str(now.hour).zfill(2) + '-' + \\\n",
        "                              str(now.minute).zfill(2) + '-' + \\\n",
        "                              str(now.second).zfill(2) + '-' + \\\n",
        "                              str(now.microsecond)  \n",
        "  \n",
        "  filename_of_img_created = data_dir + '/' + string_of_current_moment + '.jpg'\n",
        "  cv2.imwrite(filename_of_img_created, output_image)\n",
        "\n",
        "  # Append row in csv to record labelling information\n",
        "  dataset_types = ['TRAIN', 'TEST', 'VALIDATION']\n",
        "  set_type = random.choices(dataset_types, weights=(80,10,10), k=1)[0]\n",
        "  \n",
        "  for row_data in csv_row_data_placeholder:\n",
        "    (target_tile_type,min_x,min_y,max_x,max_y) = row_data\n",
        "    row_data_to_be_written_in_csv = [set_type,\n",
        "                                     filename_of_img_created,\n",
        "                                     target_tile_type,\n",
        "                                     min_x,min_y,'','',\n",
        "                                     max_x,max_y,'',''] \n",
        "    append_row_to_csv(csv_filename,row_data_to_be_written_in_csv, 'a')\n",
        "\n",
        "  return tile_type_of_tile_images_for_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EpO9vJpCBOqJ"
      },
      "outputs": [],
      "source": [
        "def genrate_images(grid_length=4):  \n",
        "\n",
        "  directory_of_github_repo_sampling_images = 'mahjong-dataset/tiles-resized/'\n",
        "\n",
        "  grid_size = grid_length * grid_length\n",
        "\n",
        "  tile_images = []\n",
        "  tile_type_of_tile_images = []\n",
        "\n",
        "  for grid_index in range(grid_size):    \n",
        "    selected_tile_type = random.choice(list(lookup_filename_of_tile_img.keys()))\n",
        "    image_samples = lookup_filename_of_tile_img[selected_tile_type]\n",
        "    selected_sample_image = cv2.imread(\n",
        "        directory_of_github_repo_sampling_images + \\\n",
        "        random.choices(image_samples, k=1)[0])\n",
        "    \n",
        "    selected_sample_image = cv2.cvtColor(selected_sample_image, \n",
        "                                         cv2.COLOR_RGB2RGBA).copy()  \n",
        "    \n",
        "    # Rotate the image\n",
        "    rotated_sample_image = rotate_tile(selected_sample_image, \n",
        "                                       random.randrange(5,355))\n",
        "\n",
        "    # The size of the image would change after rotation, \n",
        "    # and thus the rotated image has to be resized, \n",
        "    # by specifying its width to the standard width\n",
        "    resized_sample_image = \\\n",
        "    image_resize(rotated_sample_image, \n",
        "                 height=int(image_standard_height/(grid_length * 1.5)))\n",
        "    resized_sample_image = \\\n",
        "    image_resize(resized_sample_image, \n",
        "                 width=int(image_standard_width/(grid_length * 1.5)))\n",
        "    tile_images.append(resized_sample_image)\n",
        "    tile_type_of_tile_images.append(selected_tile_type)\n",
        "\n",
        "  return (tile_images, tile_type_of_tile_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Tkd9mf49sNCU"
      },
      "outputs": [],
      "source": [
        "# Check if the annotation target is met.  Annotation target is met only when \n",
        "# all tile types have the annotation number that is \n",
        "# equal or larger than the threshold\n",
        "def check_if_annotation_target_is_met(tile_type_annotations, \n",
        "                                      annotations_target_threshold):\n",
        "  for annotation_occurrence in tile_type_annotations.values():\n",
        "    if annotation_occurrence < annotations_target_threshold:\n",
        "      return False\n",
        "  \n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kdfPIWS9YKRC"
      },
      "outputs": [],
      "source": [
        "# Decalre variables to set the size of images in the dataset\n",
        "image_standard_width = image_standard_height = 1024\n",
        "\n",
        "# Declare variables to determine the size of grid.\n",
        "# E.g. for a 4x4 grid, in each image there would be 16 mahjong tiles.\n",
        "min_grid_length = 2\n",
        "max_grid_length = 8\n",
        "\n",
        "# Declare the variable to determine \n",
        "# how many annotations are needed for each tile type\n",
        "target_number_of_annotations_for_each_tile_types = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Hu90BFcVDdgV"
      },
      "outputs": [],
      "source": [
        "# Count the annotation number for determining the progress % in the progress bar\n",
        "def count_annotations_number_for_updating_progress_bar(tile_type_annotations, \n",
        "                                                      annotations_target):\n",
        "    result = 0\n",
        "    for annotation_occurrence in tile_type_annotations.values():\n",
        "      if annotation_occurrence < annotations_target:\n",
        "        result += annotation_occurrence\n",
        "      else:\n",
        "        result += annotations_target\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gC3JfwEKvajo"
      },
      "outputs": [],
      "source": [
        "# Write data to a csv file\n",
        "def append_row_to_csv(csv_filename, data, write_mode):\n",
        "  with open(csv_filename, write_mode, encoding='UTF8') \\\n",
        "  as dataset_csvfile_for_model_training:\n",
        "    writer = csv.writer(dataset_csvfile_for_model_training)\n",
        "    writer.writerow(data)\n",
        "    dataset_csvfile_for_model_training.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rf5zjfKeFKP8"
      },
      "outputs": [],
      "source": [
        "# Read labelling data from the csv file\n",
        "\n",
        "lookup_filename_of_tile_img = {}\n",
        "tile_type_annotations = {}\n",
        "\n",
        "with open('mahjong-dataset/tiles-data/data.csv', newline='') as csvfile:\n",
        "  table = csv.reader(csvfile, delimiter=' ')\n",
        "  for row in table:\n",
        "    row_data = row[0].split(',')\n",
        "    img_file = row_data[0]\n",
        "    tile_type = row_data[2]\n",
        "    if (tile_type == 'label-name' or ('bonus-' in tile_type)):\n",
        "      continue\n",
        "    if ((tile_type in tile_type_annotations) is False):\n",
        "      tile_type_annotations[tile_type] = 0\n",
        "\n",
        "    if (tile_type in lookup_filename_of_tile_img):\n",
        "      lookup_filename_of_tile_img[tile_type].append(img_file)\n",
        "    else:\n",
        "      lookup_filename_of_tile_img[tile_type] = [img_file]\n",
        "  csvfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdpgm00DWssI",
        "outputId": "9493c515-b66b-4d84-d44b-175a1fa58a9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[                                                                        ] \u001b[38;2;255;0;0m  0%\u001b[39m\n",
            "[============                                                            ] \u001b[38;2;255;104;0m 17%\u001b[39m\n",
            "[============                                                            ] \u001b[38;2;255;104;0m 17%\u001b[39m\n",
            "[=============                                                           ] \u001b[38;2;255;108;0m 18%\u001b[39m\n",
            "[==============                                                          ] \u001b[38;2;255;111;0m 20%\u001b[39m\n",
            "[===============                                                         ] \u001b[38;2;255;115;0m 21%\u001b[39m\n",
            "[================                                                        ] \u001b[38;2;255;118;0m 22%\u001b[39m\n",
            "[=================                                                       ] \u001b[38;2;255;122;0m 24%\u001b[39m\n",
            "[==================                                                      ] \u001b[38;2;255;126;0m 25%\u001b[39m\n",
            "[===================                                                     ] \u001b[38;2;255;129;0m 26%\u001b[39m\n",
            "[====================                                                    ] \u001b[38;2;255;133;0m 27%\u001b[39m\n",
            "[====================                                                    ] \u001b[38;2;255;136;0m 29%\u001b[39m\n",
            "[=====================                                                   ] \u001b[38;2;255;140;0m 30%\u001b[39m\n",
            "[======================                                                  ] \u001b[38;2;255;143;0m 31%\u001b[39m\n",
            "[=======================                                                 ] \u001b[38;2;255;147;0m 32%\u001b[39m\n",
            "[========================                                                ] \u001b[38;2;255;150;0m 34%\u001b[39m\n",
            "[=========================                                               ] \u001b[38;2;255;154;0m 35%\u001b[39m\n",
            "[==========================                                              ] \u001b[38;2;255;157;0m 36%\u001b[39m\n",
            "[===========================                                             ] \u001b[38;2;255;161;0m 38%\u001b[39m\n",
            "[============================                                            ] \u001b[38;2;255;164;0m 39%\u001b[39m\n",
            "[=============================                                           ] \u001b[38;2;255;168;0m 40%\u001b[39m\n",
            "[==============================                                          ] \u001b[38;2;255;169;0m 41%\u001b[39m\n",
            "[===============================                                         ] \u001b[38;2;255;176;0m 43%\u001b[39m\n",
            "[===============================                                         ] \u001b[38;2;255;183;0m 44%\u001b[39m\n",
            "[================================                                        ] \u001b[38;2;255;190;0m 45%\u001b[39m\n",
            "[=================================                                       ] \u001b[38;2;255;198;0m 47%\u001b[39m\n",
            "[==================================                                      ] \u001b[38;2;255;204;0m 48%\u001b[39m\n",
            "[===================================                                     ] \u001b[38;2;255;211;0m 49%\u001b[39m\n",
            "[====================================                                    ] \u001b[38;2;255;219;0m 50%\u001b[39m\n",
            "[=====================================                                   ] \u001b[38;2;255;225;0m 51%\u001b[39m\n",
            "[======================================                                  ] \u001b[38;2;255;232;0m 53%\u001b[39m\n",
            "[=======================================                                 ] \u001b[38;2;255;240;0m 54%\u001b[39m\n",
            "[========================================                                ] \u001b[38;2;255;247;0m 55%\u001b[39m\n",
            "[=========================================                               ] \u001b[38;2;255;254;0m 56%\u001b[39m\n",
            "[=========================================                               ] \u001b[38;2;255;261;0m 58%\u001b[39m\n",
            "[==========================================                              ] \u001b[38;2;248;255;0m 59%\u001b[39m\n",
            "[===========================================                             ] \u001b[38;2;244;255;0m 60%\u001b[39m\n",
            "[============================================                            ] \u001b[38;2;241;255;0m 62%\u001b[39m\n",
            "[=============================================                           ] \u001b[38;2;237;255;0m 63%\u001b[39m\n",
            "[==============================================                          ] \u001b[38;2;234;255;0m 64%\u001b[39m\n",
            "[===============================================                         ] \u001b[38;2;230;255;0m 65%\u001b[39m\n",
            "[================================================                        ] \u001b[38;2;226;255;0m 67%\u001b[39m\n",
            "[=================================================                       ] \u001b[38;2;223;255;0m 68%\u001b[39m\n",
            "[==================================================                      ] \u001b[38;2;219;255;0m 69%\u001b[39m\n",
            "[===================================================                     ] \u001b[38;2;216;255;0m 71%\u001b[39m\n",
            "[===================================================                     ] \u001b[38;2;212;255;0m 72%\u001b[39m\n",
            "[====================================================                    ] \u001b[38;2;209;255;0m 73%\u001b[39m\n",
            "[=====================================================                   ] \u001b[38;2;205;255;0m 74%\u001b[39m\n",
            "[======================================================                  ] \u001b[38;2;202;255;0m 75%\u001b[39m\n",
            "[=======================================================                 ] \u001b[38;2;198;255;0m 77%\u001b[39m\n",
            "[========================================================                ] \u001b[38;2;194;255;0m 78%\u001b[39m\n",
            "[=========================================================               ] \u001b[38;2;191;255;0m 79%\u001b[39m\n",
            "[==========================================================              ] \u001b[38;2;187;255;0m 81%\u001b[39m\n",
            "[===========================================================             ] \u001b[38;2;184;255;0m 82%\u001b[39m\n",
            "[============================================================            ] \u001b[38;2;180;255;0m 83%\u001b[39m\n",
            "[=============================================================           ] \u001b[38;2;177;255;0m 84%\u001b[39m\n",
            "[==============================================================          ] \u001b[38;2;173;255;0m 86%\u001b[39m\n",
            "[==============================================================          ] \u001b[38;2;170;255;0m 87%\u001b[39m\n",
            "[===============================================================         ] \u001b[38;2;166;255;0m 88%\u001b[39m\n",
            "[================================================================        ] \u001b[38;2;163;255;0m 89%\u001b[39m\n",
            "[=================================================================       ] \u001b[38;2;159;255;0m 91%\u001b[39m\n",
            "[==================================================================      ] \u001b[38;2;90;255;0m 92%\u001b[39m\n",
            "[===================================================================     ] \u001b[38;2;77;255;0m 93%\u001b[39m\n",
            "[====================================================================    ] \u001b[38;2;60;255;0m 95%\u001b[39m\n",
            "[=====================================================================   ] \u001b[38;2;46;255;0m 96%\u001b[39m\n",
            "[======================================================================  ] \u001b[38;2;30;255;0m 97%\u001b[39m\n",
            "[======================================================================= ] \u001b[38;2;15;255;0m 98%\u001b[39m\n",
            "[========================================================================] \u001b[38;2;0;255;0m100%\u001b[39m\n",
            "[========================================================================] \u001b[38;2;0;255;0m100%\u001b[39m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of images generated: 993\n",
            "Total number of annotations created: 35808\n"
          ]
        }
      ],
      "source": [
        "# Create a csv file that would contain the annotation information of the dataset images\n",
        "csv_filename = 'dataset-for-training-AutoML-Vision-model.csv'\n",
        "header = ['set','path','label','x_min','y_min','x_max',\n",
        "          'y_min','x_max','y_max','x_min','y_max']\n",
        "append_row_to_csv(csv_filename, header, 'w')\n",
        "\n",
        "# Create a progress bar to display progress\n",
        "total_number_of_tile_types = 34\n",
        "progress_bar_max_val = total_number_of_tile_types * \\\n",
        "                        target_number_of_annotations_for_each_tile_types\n",
        "\n",
        "bar = progressbar.ProgressBar(max_value=progress_bar_max_val, \n",
        "                              min_value=0, \n",
        "                              widgets=[progressbar.Bar('=', '[', ']'),\n",
        "                                       ' ', \n",
        "                                       progressbar.Percentage()])\n",
        "annotations_number_for_updaing_progress_bar = 0\n",
        "bar.start()\n",
        "bar.update(annotations_number_for_updaing_progress_bar)\n",
        "\n",
        "# Generate images until the number of annotation target is met\n",
        "annotation_target_is_met = False\n",
        "total_number_of_images_generated = 0\n",
        "while not annotation_target_is_met:  \n",
        "  grid_length = random.randint(min_grid_length, max_grid_length)\n",
        "  tile_types_annotated = create_image_in_grid_format(csv_filename, \n",
        "                                                     grid_length)\n",
        "  total_number_of_images_generated += 1\n",
        "  for tile_type in tile_types_annotated:\n",
        "    tile_type_annotations[tile_type] += 1\n",
        "  annotations_number_for_updaing_progress_bar = \\\n",
        "  count_annotations_number_for_updating_progress_bar(\n",
        "      tile_type_annotations, \n",
        "      target_number_of_annotations_for_each_tile_types)\n",
        "  bar.update(annotations_number_for_updaing_progress_bar)\n",
        "  annotation_target_is_met = \\\n",
        "  check_if_annotation_target_is_met(tile_type_annotations, target_number_of_annotations_for_each_tile_types)\n",
        "\n",
        "bar.finish()\n",
        "print(\"Total number of images generated: \" + str(total_number_of_images_generated))\n",
        "\n",
        "total_number_of_annotations = sum(tile_type_annotations.values())\n",
        "print(\"Total number of annotations created: \" + str(total_number_of_annotations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Auto ML to Yolo Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "# from collections import defaultdict\n",
        "\n",
        "# Define file paths\n",
        "csv_file_path = 'dataset-for-training-AutoML-Vision-model.csv'\n",
        "output_directory = 'training/yolo_format/'\n",
        "class_mapping_file_path = 'yolo_class_mapping.txt'\n",
        "\n",
        "# Make sure the output directory exists\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Initialize a set to hold unique class names\n",
        "class_names = set()\n",
        "with open(csv_file_path, newline='') as csvfile:\n",
        "    reader = csv.reader((line.replace(',,', ', ,') for line in csvfile))  # Replace 'empty' fields marked by ',,' with ', ,'\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        class_names.add(row[2])  # Add class label to the set\n",
        "\n",
        "# Convert class names set to a list and sort it for consistent class index mapping\n",
        "class_names = sorted(list(class_names))\n",
        "\n",
        "# Create a dictionary to map class names to indices\n",
        "class_mapping = {name: index for index, name in enumerate(class_names)}\n",
        "\n",
        "# Process the CSV file to convert to YOLO format\n",
        "with open(csv_file_path, newline='') as csvfile:\n",
        "    reader = csv.reader((line.replace(',,', ', ,') for line in csvfile))  # Replace 'empty' fields marked by ',,' with ', ,'\n",
        "    next(reader)  # Skip the header\n",
        "    \n",
        "    for row in reader:\n",
        "        set_type = row[0].lower()\n",
        "        img_file = os.path.basename(row[1])\n",
        "        class_label = row[2]\n",
        "        x_min = float(row[3])\n",
        "        y_min = float(row[4])\n",
        "        x_max = float(row[7])\n",
        "        y_max = float(row[8])\n",
        "\n",
        "        x_center = (x_min + x_max) / 2\n",
        "        y_center = (y_min + y_max) / 2\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        class_index = class_mapping[class_label]\n",
        "\n",
        "        txt_file_path = os.path.join(output_directory, f\"{set_type}_labels\", img_file.replace('.jpg', '.txt'))\n",
        "        os.makedirs(os.path.dirname(txt_file_path), exist_ok=True)\n",
        "        \n",
        "        with open(txt_file_path, 'a') as yolo_file:\n",
        "            yolo_file.write(f\"{class_index} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "# Save the class mappings to a text file so you have a reference to which indices correspond to which classes\n",
        "\n",
        "with open(os.path.join(output_directory, class_mapping_file_path), 'w') as class_file:\n",
        "    for class_index, class_name in enumerate(class_names):\n",
        "        class_file.write(f\"{class_name}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clone yolo and start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone https://github.com/ultralytics/yolov5.git\n",
        "    cd yolov5\n",
        "    pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying train: \u001b[38;2;255;0;0m  0%\u001b[39m |/                                         | ETA:  --:--:--\n",
            "Copying train: \u001b[38;2;255;104;0m 17%\u001b[39m |-                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;255;154;0m 35%\u001b[39m |\\                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;255;239;0m 54%\u001b[39m ||                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;212;255;0m 72%\u001b[39m |/                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;159;255;0m 91%\u001b[39m |-                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;0;255;0m100%\u001b[39m ||                                         | Time:  0:00:00\n",
            "Copying val: \u001b[38;2;255;0;0m  0%\u001b[39m |/                                           | ETA:  --:--:--\n",
            "Copying val: \u001b[38;2;0;255;0m100%\u001b[39m ||                                           | Time:  0:00:00\n",
            "Copying test: \u001b[38;2;255;0;0m  0%\u001b[39m |/                                          | ETA:  --:--:--\n",
            "Copying test: \u001b[38;2;0;255;0m100%\u001b[39m ||                                          | Time:  0:00:00\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define your current directories\n",
        "labels_directory = 'training/yolo_format'\n",
        "images_directory = 'images'  # Assuming this contains all images (unsorted)\n",
        "\n",
        "# Define the base directory for the YOLOv5 project\n",
        "yolov5_base_directory = './yolov5'\n",
        "dataset_name = 'mahjong'  # Name of your dataset\n",
        "\n",
        "# Define the new paths for images and labels\n",
        "new_dataset_base = os.path.join(yolov5_base_directory, 'datasets', dataset_name)\n",
        "new_images_base = os.path.join(new_dataset_base, 'images')\n",
        "new_labels_base = os.path.join(new_dataset_base, 'labels')\n",
        "\n",
        "# Set names for train, val, and test sets\n",
        "sets = ['train', 'val', 'test']\n",
        "\n",
        "# Create the directories for images and labels based on sets if they do not exist\n",
        "for set_name in sets:\n",
        "    os.makedirs(os.path.join(new_images_base, set_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(new_labels_base, set_name), exist_ok=True)\n",
        "\n",
        "# Function to copy labels and images with a progress bar\n",
        "def copy_labels_and_images_with_progress(label_set_name, src_labels_dir, dest_images_dir, dest_labels_dir):\n",
        "    if not os.path.exists(src_labels_dir):\n",
        "        print(f\"No labels directory found for {label_set_name}, skipping.\")\n",
        "        return\n",
        "\n",
        "    label_files = os.listdir(src_labels_dir)\n",
        "    widgets = [\n",
        "        f'Copying {label_set_name}: ', progressbar.Percentage(), ' ',\n",
        "        progressbar.Bar(marker=progressbar.RotatingMarker()), ' ', progressbar.ETA()\n",
        "    ]\n",
        "    bar = progressbar.ProgressBar(widgets=widgets, max_value=len(label_files)).start()\n",
        "\n",
        "    for i, label_file in enumerate(label_files):\n",
        "        # Copy label file\n",
        "        src_label_path = os.path.join(src_labels_dir, label_file)\n",
        "        dest_label_path = os.path.join(dest_labels_dir, label_file)\n",
        "        shutil.copy(src_label_path, dest_label_path)\n",
        "\n",
        "        # Determine corresponding image file and copy it\n",
        "        image_file_name = label_file.replace('.txt', '.jpg')  # Assumes images are .jpg\n",
        "        src_image_path = os.path.join(images_directory, image_file_name)\n",
        "        dest_image_path = os.path.join(dest_images_dir, image_file_name)\n",
        "        if os.path.exists(src_image_path):\n",
        "            shutil.copy(src_image_path, dest_image_path)\n",
        "        else:\n",
        "            print(f\"Warning: Corresponding image file not found for {src_image_path}\")\n",
        "\n",
        "        bar.update(i+1)\n",
        "    bar.finish()\n",
        "\n",
        "# Copy labels and images for each set using the progress bar\n",
        "for set_name in sets:\n",
        "    copy_labels_and_images_with_progress(\n",
        "        label_set_name=set_name,\n",
        "        src_labels_dir=os.path.join(labels_directory, f\"{set_name}_labels\"),\n",
        "        dest_images_dir=os.path.join(new_images_base, set_name),\n",
        "        dest_labels_dir=os.path.join(new_labels_base, set_name),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cp mahjong.yaml yolov5/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=./models/yolov5s.yaml, data=mahjong.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=1024, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=mps, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=mahjong_train, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-294-gdb125a20 Python-3.12.1 torch-2.2.1 MPS\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=34\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    105183  models.yolo.Detect                      [34, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7111327 parameters, 7111327 gradients, 16.2 GFLOPs\n",
            "\n",
            "Transferred 342/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/yolov5/datasets/mahjong/labels/train.cache... 801 images, 0 backgrounds, 0 corrupt: 100%|██████████| 801/801 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/yolov5/datasets/mahjong/labels/val.cache... 89 images, 0 backgrounds, 0 corrupt: 100%|██████████| 89/89 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.54 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/mahjong_train2/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/mahjong_train2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "  0%|          | 0/51 [00:00<?, ?it/s]/AppleInternal/Library/BuildRoots/ce725a5f-c761-11ee-a4ec-b6ef2fd8d87b/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:124: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (3) is not less than length of dimension[1] (3)'\n",
            "bash: line 2: 44644 Abort trap: 6           python train.py --device mps --img 1024 --batch 16 --epochs 100 --data mahjong.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name mahjong_train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cd yolov5/\n",
        "python train.py --include coreml --img 1024 --batch 16 --epochs 100 --data mahjong.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name mahjong_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.1.27 🚀 Python-3.12.1 torch-2.2.1 MPS (Apple M1 Max)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=mahjong_yolov8.yaml, epochs=300, time=None, patience=100, batch=64, imgsz=1024, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=train14, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/xingfanxia/projects/mahjong_hand_classifier/runs/detect/train14\n",
            "Overriding model.yaml nc=80 with nc=34\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    757942  ultralytics.nn.modules.head.Detect           [34, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3017478 parameters, 3017462 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/datasets/mahjong/labels/train.cache... 801 images, 0 backgrounds, 0 corrupt: 100%|██████████| 801/801 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/datasets/mahjong/labels/val.cache... 89 images, 0 backgrounds, 0 corrupt: 100%|██████████| 89/89 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to /Users/xingfanxia/projects/mahjong_hand_classifier/runs/detect/train14/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000263, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/Users/xingfanxia/projects/mahjong_hand_classifier/runs/detect/train14\u001b[0m\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      1/300         0G          0       1311          0       2615       1024:   8%|▊         | 1/13 [01:23<16:41, 83.47s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmahjong_yolov8.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mval()  \u001b[38;5;66;03m# evaluate model performance on the validation set\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\u001b[39;00m\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:655\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:213\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:393\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m     last_opt_step \u001b[38;5;241m=\u001b[39m ni\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:535\u001b[0m, in \u001b[0;36mBaseTrainer.optimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[1;32m    534\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)  \u001b[38;5;66;03m# clip gradients\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:378\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m     )\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    177\u001b[0m         group,\n\u001b[1;32m    178\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m         state_steps,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:339\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 339\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/projects/mahjong_hand_classifier/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:419\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    418\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 419\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    422\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "model.train(data=\"mahjong_yolov8.yaml\", imgsz=1024, epochs=3, batch=128, device=\"mps\")  # train the model\n",
        "metrics = model.val()  # evaluate model performance on the validation set\n",
        "# results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
        "onnx_path = model.export(format=\"onnx\")  # export the model to ONNX format\n",
        "coreml_path = model.export(format=\"coreml\")  # export the model to CoreML format\n",
        "tflite_path = model.export(format=\"tflite\")  # export the model to TensorFlow Lite format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mahjong-dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
