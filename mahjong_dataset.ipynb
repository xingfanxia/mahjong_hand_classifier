{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJMCMHwFcC1I"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kGWIFVVPaMx3"
      },
      "outputs": [],
      "source": [
        "from glob import glob \n",
        "# from google.colab import files\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import numpy as np\n",
        "import cv2\n",
        "import progressbar\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=EBEL31R58IXWY4L66CN6\n",
            "env: CLEARML_API_SECRET_KEY=9aR1biMbd2ZMjzjlbFhl89MrOiaszqDqAif7n0YFXS9FQzD6T8\n"
          ]
        }
      ],
      "source": [
        "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=EBEL31R58IXWY4L66CN6\n",
        "%env CLEARML_API_SECRET_KEY=9aR1biMbd2ZMjzjlbFhl89MrOiaszqDqAif7n0YFXS9FQzD6T8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ClearML Task: overwriting (reusing) task id=079ef3b3e7a44d0e95c5711019271bea\n",
            "2024-03-12 18:54:19,944 - clearml.Repository Detection - WARNING - Could not read Jupyter Notebook: No module named 'nbconvert'\n",
            "2024-03-12 18:54:19,976 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
            "ClearML results page: https://app.clear.ml/projects/7542bb1e5d0c45f7a082eee181537970/experiments/079ef3b3e7a44d0e95c5711019271bea/output/log\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
            "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
          ]
        }
      ],
      "source": [
        "from clearml import Task\n",
        "\n",
        "task = Task.init(project_name='mahjong yolov8', task_name='yolov8_cpu_run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Valid Comet API Key saved in /Users/xingfanxia/.comet.config (set COMET_CONFIG to change where it is saved).\n"
          ]
        }
      ],
      "source": [
        "import comet_ml\n",
        "comet_ml.init(api_key=\"QbMOBXg9MxnhPgJyPv1eSG7Sp\", project_name='mahjong_yolov8_cpu')\n",
        "\n",
        "# experiment = Experiment(\n",
        "#     api_key=\"QbMOBXg9MxnhPgJyPv1eSG7Sp\",\n",
        "#     project_name=\"mahjong_yolov8\",\n",
        "#     workspace=\"mahjong_detection\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz-TXokVshuI"
      },
      "source": [
        "**Retrieve images from external sources**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xxnba9tyEDT",
        "outputId": "b12b92e7-82be-470e-be27-45fce4ee5aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mahjong-dataset' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone the GitHub repo of mahjong tile images\n",
        "!git clone https://github.com/camerash/mahjong-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqj--Gy6g0YQ",
        "outputId": "2b3816eb-e4e0-4566-f618-af702504d2bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-11 22:44:08--  https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/datasets/dtd/dtd-r1.0.1.tar.gz [following]\n",
            "--2024-03-11 22:44:09--  https://thor.robots.ox.ac.uk/datasets/dtd/dtd-r1.0.1.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 625239812 (596M) [application/octet-stream]\n",
            "Saving to: â€˜dtd-r1.0.1.tar.gzâ€™\n",
            "\n",
            "dtd-r1.0.1.tar.gz   100%[===================>] 596.27M  24.3MB/s    in 26s     \n",
            "\n",
            "2024-03-11 22:44:35 (23.3 MB/s) - â€˜dtd-r1.0.1.tar.gzâ€™ saved [625239812/625239812]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Describable Textures Dataset (DTD)\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fzkm0eIpg0yW"
      },
      "outputs": [],
      "source": [
        "# Extract the DTD\n",
        "!tar xf dtd-r1.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fdZLUYX-g154"
      },
      "outputs": [],
      "source": [
        "# Delete the zip folder of DTD\n",
        "!rm dtd-r1.0.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-K4nMUBg49U",
        "outputId": "5e0ee940-afec-4dc0-9a05-ffa4f94d987b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading images ... (It could take several minutes)\n",
            "Number of images loaded : 5640\n"
          ]
        }
      ],
      "source": [
        "# Load all *jpg from dtd subdirectories and save them in a pickle file (1x)\n",
        "\n",
        "backgrounds_pck_fn=\"backgrounds.pck\"\n",
        "dtd_dir=\"dtd/images/\"\n",
        "bg_images=[]\n",
        "print(\"Loading images ... (It could take several minutes)\")\n",
        "for subdir in glob(dtd_dir+\"/*\"):\n",
        "    for f in glob(subdir+\"/*.jpg\"):\n",
        "        bg_images.append(mpimg.imread(f))\n",
        "pickle.dump(bg_images,open(backgrounds_pck_fn,'wb'))\n",
        "\n",
        "class Backgrounds():\n",
        "    def __init__(self,backgrounds_pck_fn=backgrounds_pck_fn):\n",
        "        self._images=pickle.load(open(backgrounds_pck_fn,'rb'))\n",
        "        self._nb_images=len(self._images)\n",
        "        print(\"Number of images loaded :\", self._nb_images)\n",
        "    def get_random(self, display=False):\n",
        "        bg=self._images[random.randint(0,self._nb_images-1)]\n",
        "        if display: plt.imshow(bg)\n",
        "        return bg\n",
        "\n",
        "backgrounds = Backgrounds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrmhNjM4dQvt"
      },
      "source": [
        "**Edit directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LhOzuvRR5Stx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: images: File exists\n"
          ]
        }
      ],
      "source": [
        "# Create a directory that will contain the images we generate\n",
        "data_dir=\"images\"\n",
        "!mkdir images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJBBYGpbdTKF"
      },
      "source": [
        "**Define functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CskbcMZG6M9L"
      },
      "outputs": [],
      "source": [
        "# Rotate the image to the angle as specified\n",
        "def rotate_tile(image, angle):\n",
        "    # grab the dimensions of the image and then determine the\n",
        "    # center\n",
        "    (h, w) = image.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    border_color=(255, 255, 255)\n",
        "\n",
        "    # grab the rotation matrix (applying the negative of the\n",
        "    # angle to rotate clockwise), then grab the sine and cosine\n",
        "    # (i.e. the rotation components of the matrix)\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        " \n",
        "    # compute the new bounding dimensions of the image\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        " \n",
        "    # adjust the rotation matrix to take into account translation\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        " \n",
        "    # perform the actual rotation and return the image\n",
        "    return cv2.warpAffine(image, M, (nW, nH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mkuReCFD_7CT"
      },
      "outputs": [],
      "source": [
        "# Resize an image\n",
        "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
        "    # initialize the dimensions of the image to be resized and\n",
        "    # grab the image size\n",
        "    dim = None\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    # if both the width and height are None, then return the\n",
        "    # original image\n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "\n",
        "    # check to see if the width is None\n",
        "    if width is None:\n",
        "        # calculate the ratio of the height and construct the\n",
        "        # dimensions\n",
        "        r = height / float(h)\n",
        "        dim = (int(w * r), height)\n",
        "\n",
        "    # otherwise, the height is None\n",
        "    else:\n",
        "        # calculate the ratio of the width and construct the\n",
        "        # dimensions\n",
        "        r = width / float(w)\n",
        "        dim = (width, int(h * r))\n",
        "\n",
        "    # resize the image\n",
        "    resized = cv2.resize(image, dim, interpolation = inter)\n",
        "\n",
        "    # return the resized image\n",
        "    return resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vCe_1VC9pyFm"
      },
      "outputs": [],
      "source": [
        "def create_image_in_grid_format(csv_filename, grid_length=4):\n",
        "  (tile_images, tile_type_of_tile_images) = genrate_images(grid_length)\n",
        "  tile_type_of_tile_images_for_output = tile_type_of_tile_images[:]\n",
        "\n",
        "  # Randomly select a background image\n",
        "  background_image = backgrounds.get_random()\n",
        "\n",
        "  # Copy the background image such that the image would not be mutated  \n",
        "  output_image = cv2.cvtColor(background_image, cv2.COLOR_RGB2RGBA).copy()\n",
        "  \n",
        "  # Resize the background image to standard width and height (i.e. 1024x1024)\n",
        "  # The resize action would result int distortion\n",
        "  resize_specifications = (image_standard_width, image_standard_height)\n",
        "  output_image = cv2.resize(output_image, resize_specifications, \n",
        "                            interpolation = cv2.INTER_AREA)\n",
        "  csv_row_data_placeholder = []\n",
        "\n",
        "  # Place the grid on the resized background image, \n",
        "  # where the black area of the grid would be perceived as transparent\n",
        "  for row_index in range(grid_length): \n",
        "    for column_index in range(grid_length):\n",
        "      tile_img = tile_images[0]\n",
        "      tmp = cv2.cvtColor(tile_img, cv2.COLOR_BGR2GRAY)\n",
        "      _,alpha = cv2.threshold(tmp,0,255,cv2.THRESH_BINARY)\n",
        "      b, g, r, a = cv2.split(tile_img)\n",
        "      rgba = [b,g,r, alpha]\n",
        "      tile_img = cv2.merge(rgba,4)\n",
        "\n",
        "      x_offset = int(((column_index / grid_length) * output_image.shape[1]))\n",
        "      y_offset = int((((row_index) / grid_length) * output_image.shape[0]))\n",
        "\n",
        "      y1, y2 = y_offset, y_offset + tile_img.shape[0]\n",
        "      x1, x2 = x_offset, x_offset + tile_img.shape[1]\n",
        "      \n",
        "      alpha_s = tile_img[:, :, 3] / 255.0\n",
        "      alpha_l = 1.0 - alpha_s\n",
        "      \n",
        "      for c in range(0, 3):\n",
        "        output_image[y1:y2, x1:x2, c] = (alpha_s * tile_img[:, :, c] + alpha_l \n",
        "                                         * output_image[y1:y2, x1:x2, c])\n",
        "        \n",
        "      # Compute the value of relative coordinates\n",
        "      min_x = x1/output_image.shape[1]\n",
        "      max_x = x2/output_image.shape[1]\n",
        "      min_y = y1/output_image.shape[0]\n",
        "      max_y = y2/output_image.shape[0]\n",
        "\n",
        "      # Write data to csv: \n",
        "      # (1) the image filename, \n",
        "      # (2) tile type and \n",
        "      # (3) top-left and bottom-right vertices coordinates \n",
        "      #     of the mahjong tile in the image \n",
        "      csv_row_data_placeholder.append(\n",
        "          (tile_type_of_tile_images[0],min_x,min_y,max_x,max_y))\n",
        "      \n",
        "      del tile_images[0]\n",
        "      del tile_type_of_tile_images[0]\n",
        "\n",
        "  # Save the image\n",
        "  now = datetime.datetime.now()\n",
        "  \n",
        "  string_of_current_moment = str(now.year) + '-' + \\\n",
        "                              str(now.month).zfill(2) + '-' + \\\n",
        "                              str(now.day).zfill(2) + '-'  + \\\n",
        "                              str(now.hour).zfill(2) + '-' + \\\n",
        "                              str(now.minute).zfill(2) + '-' + \\\n",
        "                              str(now.second).zfill(2) + '-' + \\\n",
        "                              str(now.microsecond)  \n",
        "  \n",
        "  filename_of_img_created = data_dir + '/' + string_of_current_moment + '.jpg'\n",
        "  cv2.imwrite(filename_of_img_created, output_image)\n",
        "\n",
        "  # Append row in csv to record labelling information\n",
        "  dataset_types = ['TRAIN', 'TEST', 'VALIDATION']\n",
        "  set_type = random.choices(dataset_types, weights=(80,10,10), k=1)[0]\n",
        "  \n",
        "  for row_data in csv_row_data_placeholder:\n",
        "    (target_tile_type,min_x,min_y,max_x,max_y) = row_data\n",
        "    row_data_to_be_written_in_csv = [set_type,\n",
        "                                     filename_of_img_created,\n",
        "                                     target_tile_type,\n",
        "                                     min_x,min_y,'','',\n",
        "                                     max_x,max_y,'',''] \n",
        "    append_row_to_csv(csv_filename,row_data_to_be_written_in_csv, 'a')\n",
        "\n",
        "  return tile_type_of_tile_images_for_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EpO9vJpCBOqJ"
      },
      "outputs": [],
      "source": [
        "def genrate_images(grid_length=4):  \n",
        "\n",
        "  directory_of_github_repo_sampling_images = 'mahjong-dataset/tiles-resized/'\n",
        "\n",
        "  grid_size = grid_length * grid_length\n",
        "\n",
        "  tile_images = []\n",
        "  tile_type_of_tile_images = []\n",
        "\n",
        "  for grid_index in range(grid_size):    \n",
        "    selected_tile_type = random.choice(list(lookup_filename_of_tile_img.keys()))\n",
        "    image_samples = lookup_filename_of_tile_img[selected_tile_type]\n",
        "    selected_sample_image = cv2.imread(\n",
        "        directory_of_github_repo_sampling_images + \\\n",
        "        random.choices(image_samples, k=1)[0])\n",
        "    \n",
        "    selected_sample_image = cv2.cvtColor(selected_sample_image, \n",
        "                                         cv2.COLOR_RGB2RGBA).copy()  \n",
        "    \n",
        "    # Rotate the image\n",
        "    rotated_sample_image = rotate_tile(selected_sample_image, \n",
        "                                       random.randrange(5,355))\n",
        "\n",
        "    # The size of the image would change after rotation, \n",
        "    # and thus the rotated image has to be resized, \n",
        "    # by specifying its width to the standard width\n",
        "    resized_sample_image = \\\n",
        "    image_resize(rotated_sample_image, \n",
        "                 height=int(image_standard_height/(grid_length * 1.5)))\n",
        "    resized_sample_image = \\\n",
        "    image_resize(resized_sample_image, \n",
        "                 width=int(image_standard_width/(grid_length * 1.5)))\n",
        "    tile_images.append(resized_sample_image)\n",
        "    tile_type_of_tile_images.append(selected_tile_type)\n",
        "\n",
        "  return (tile_images, tile_type_of_tile_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Tkd9mf49sNCU"
      },
      "outputs": [],
      "source": [
        "# Check if the annotation target is met.  Annotation target is met only when \n",
        "# all tile types have the annotation number that is \n",
        "# equal or larger than the threshold\n",
        "def check_if_annotation_target_is_met(tile_type_annotations, \n",
        "                                      annotations_target_threshold):\n",
        "  for annotation_occurrence in tile_type_annotations.values():\n",
        "    if annotation_occurrence < annotations_target_threshold:\n",
        "      return False\n",
        "  \n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kdfPIWS9YKRC"
      },
      "outputs": [],
      "source": [
        "# Decalre variables to set the size of images in the dataset\n",
        "image_standard_width = image_standard_height = 1024\n",
        "\n",
        "# Declare variables to determine the size of grid.\n",
        "# E.g. for a 4x4 grid, in each image there would be 16 mahjong tiles.\n",
        "min_grid_length = 2\n",
        "max_grid_length = 8\n",
        "\n",
        "# Declare the variable to determine \n",
        "# how many annotations are needed for each tile type\n",
        "target_number_of_annotations_for_each_tile_types = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Hu90BFcVDdgV"
      },
      "outputs": [],
      "source": [
        "# Count the annotation number for determining the progress % in the progress bar\n",
        "def count_annotations_number_for_updating_progress_bar(tile_type_annotations, \n",
        "                                                      annotations_target):\n",
        "    result = 0\n",
        "    for annotation_occurrence in tile_type_annotations.values():\n",
        "      if annotation_occurrence < annotations_target:\n",
        "        result += annotation_occurrence\n",
        "      else:\n",
        "        result += annotations_target\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gC3JfwEKvajo"
      },
      "outputs": [],
      "source": [
        "# Write data to a csv file\n",
        "def append_row_to_csv(csv_filename, data, write_mode):\n",
        "  with open(csv_filename, write_mode, encoding='UTF8') \\\n",
        "  as dataset_csvfile_for_model_training:\n",
        "    writer = csv.writer(dataset_csvfile_for_model_training)\n",
        "    writer.writerow(data)\n",
        "    dataset_csvfile_for_model_training.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rf5zjfKeFKP8"
      },
      "outputs": [],
      "source": [
        "# Read labelling data from the csv file\n",
        "\n",
        "lookup_filename_of_tile_img = {}\n",
        "tile_type_annotations = {}\n",
        "\n",
        "with open('mahjong-dataset/tiles-data/data.csv', newline='') as csvfile:\n",
        "  table = csv.reader(csvfile, delimiter=' ')\n",
        "  for row in table:\n",
        "    row_data = row[0].split(',')\n",
        "    img_file = row_data[0]\n",
        "    tile_type = row_data[2]\n",
        "    if (tile_type == 'label-name' or ('bonus-' in tile_type)):\n",
        "      continue\n",
        "    if ((tile_type in tile_type_annotations) is False):\n",
        "      tile_type_annotations[tile_type] = 0\n",
        "\n",
        "    if (tile_type in lookup_filename_of_tile_img):\n",
        "      lookup_filename_of_tile_img[tile_type].append(img_file)\n",
        "    else:\n",
        "      lookup_filename_of_tile_img[tile_type] = [img_file]\n",
        "  csvfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdpgm00DWssI",
        "outputId": "9493c515-b66b-4d84-d44b-175a1fa58a9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[                                                                        ] \u001b[38;2;255;0;0m  0%\u001b[39m\n",
            "[============                                                            ] \u001b[38;2;255;104;0m 17%\u001b[39m\n",
            "[============                                                            ] \u001b[38;2;255;104;0m 17%\u001b[39m\n",
            "[=============                                                           ] \u001b[38;2;255;108;0m 18%\u001b[39m\n",
            "[==============                                                          ] \u001b[38;2;255;111;0m 20%\u001b[39m\n",
            "[===============                                                         ] \u001b[38;2;255;115;0m 21%\u001b[39m\n",
            "[================                                                        ] \u001b[38;2;255;118;0m 22%\u001b[39m\n",
            "[=================                                                       ] \u001b[38;2;255;122;0m 24%\u001b[39m\n",
            "[==================                                                      ] \u001b[38;2;255;126;0m 25%\u001b[39m\n",
            "[===================                                                     ] \u001b[38;2;255;129;0m 26%\u001b[39m\n",
            "[====================                                                    ] \u001b[38;2;255;133;0m 27%\u001b[39m\n",
            "[====================                                                    ] \u001b[38;2;255;136;0m 29%\u001b[39m\n",
            "[=====================                                                   ] \u001b[38;2;255;140;0m 30%\u001b[39m\n",
            "[======================                                                  ] \u001b[38;2;255;143;0m 31%\u001b[39m\n",
            "[=======================                                                 ] \u001b[38;2;255;147;0m 32%\u001b[39m\n",
            "[========================                                                ] \u001b[38;2;255;150;0m 34%\u001b[39m\n",
            "[=========================                                               ] \u001b[38;2;255;154;0m 35%\u001b[39m\n",
            "[==========================                                              ] \u001b[38;2;255;157;0m 36%\u001b[39m\n",
            "[===========================                                             ] \u001b[38;2;255;161;0m 38%\u001b[39m\n",
            "[============================                                            ] \u001b[38;2;255;164;0m 39%\u001b[39m\n",
            "[=============================                                           ] \u001b[38;2;255;168;0m 40%\u001b[39m\n",
            "[==============================                                          ] \u001b[38;2;255;169;0m 41%\u001b[39m\n",
            "[===============================                                         ] \u001b[38;2;255;176;0m 43%\u001b[39m\n",
            "[===============================                                         ] \u001b[38;2;255;183;0m 44%\u001b[39m\n",
            "[================================                                        ] \u001b[38;2;255;190;0m 45%\u001b[39m\n",
            "[=================================                                       ] \u001b[38;2;255;198;0m 47%\u001b[39m\n",
            "[==================================                                      ] \u001b[38;2;255;204;0m 48%\u001b[39m\n",
            "[===================================                                     ] \u001b[38;2;255;211;0m 49%\u001b[39m\n",
            "[====================================                                    ] \u001b[38;2;255;219;0m 50%\u001b[39m\n",
            "[=====================================                                   ] \u001b[38;2;255;225;0m 51%\u001b[39m\n",
            "[======================================                                  ] \u001b[38;2;255;232;0m 53%\u001b[39m\n",
            "[=======================================                                 ] \u001b[38;2;255;240;0m 54%\u001b[39m\n",
            "[========================================                                ] \u001b[38;2;255;247;0m 55%\u001b[39m\n",
            "[=========================================                               ] \u001b[38;2;255;254;0m 56%\u001b[39m\n",
            "[=========================================                               ] \u001b[38;2;255;261;0m 58%\u001b[39m\n",
            "[==========================================                              ] \u001b[38;2;248;255;0m 59%\u001b[39m\n",
            "[===========================================                             ] \u001b[38;2;244;255;0m 60%\u001b[39m\n",
            "[============================================                            ] \u001b[38;2;241;255;0m 62%\u001b[39m\n",
            "[=============================================                           ] \u001b[38;2;237;255;0m 63%\u001b[39m\n",
            "[==============================================                          ] \u001b[38;2;234;255;0m 64%\u001b[39m\n",
            "[===============================================                         ] \u001b[38;2;230;255;0m 65%\u001b[39m\n",
            "[================================================                        ] \u001b[38;2;226;255;0m 67%\u001b[39m\n",
            "[=================================================                       ] \u001b[38;2;223;255;0m 68%\u001b[39m\n",
            "[==================================================                      ] \u001b[38;2;219;255;0m 69%\u001b[39m\n",
            "[===================================================                     ] \u001b[38;2;216;255;0m 71%\u001b[39m\n",
            "[===================================================                     ] \u001b[38;2;212;255;0m 72%\u001b[39m\n",
            "[====================================================                    ] \u001b[38;2;209;255;0m 73%\u001b[39m\n",
            "[=====================================================                   ] \u001b[38;2;205;255;0m 74%\u001b[39m\n",
            "[======================================================                  ] \u001b[38;2;202;255;0m 75%\u001b[39m\n",
            "[=======================================================                 ] \u001b[38;2;198;255;0m 77%\u001b[39m\n",
            "[========================================================                ] \u001b[38;2;194;255;0m 78%\u001b[39m\n",
            "[=========================================================               ] \u001b[38;2;191;255;0m 79%\u001b[39m\n",
            "[==========================================================              ] \u001b[38;2;187;255;0m 81%\u001b[39m\n",
            "[===========================================================             ] \u001b[38;2;184;255;0m 82%\u001b[39m\n",
            "[============================================================            ] \u001b[38;2;180;255;0m 83%\u001b[39m\n",
            "[=============================================================           ] \u001b[38;2;177;255;0m 84%\u001b[39m\n",
            "[==============================================================          ] \u001b[38;2;173;255;0m 86%\u001b[39m\n",
            "[==============================================================          ] \u001b[38;2;170;255;0m 87%\u001b[39m\n",
            "[===============================================================         ] \u001b[38;2;166;255;0m 88%\u001b[39m\n",
            "[================================================================        ] \u001b[38;2;163;255;0m 89%\u001b[39m\n",
            "[=================================================================       ] \u001b[38;2;159;255;0m 91%\u001b[39m\n",
            "[==================================================================      ] \u001b[38;2;90;255;0m 92%\u001b[39m\n",
            "[===================================================================     ] \u001b[38;2;77;255;0m 93%\u001b[39m\n",
            "[====================================================================    ] \u001b[38;2;60;255;0m 95%\u001b[39m\n",
            "[=====================================================================   ] \u001b[38;2;46;255;0m 96%\u001b[39m\n",
            "[======================================================================  ] \u001b[38;2;30;255;0m 97%\u001b[39m\n",
            "[======================================================================= ] \u001b[38;2;15;255;0m 98%\u001b[39m\n",
            "[========================================================================] \u001b[38;2;0;255;0m100%\u001b[39m\n",
            "[========================================================================] \u001b[38;2;0;255;0m100%\u001b[39m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of images generated: 993\n",
            "Total number of annotations created: 35808\n"
          ]
        }
      ],
      "source": [
        "# Create a csv file that would contain the annotation information of the dataset images\n",
        "csv_filename = 'dataset-for-training-AutoML-Vision-model.csv'\n",
        "header = ['set','path','label','x_min','y_min','x_max',\n",
        "          'y_min','x_max','y_max','x_min','y_max']\n",
        "append_row_to_csv(csv_filename, header, 'w')\n",
        "\n",
        "# Create a progress bar to display progress\n",
        "total_number_of_tile_types = 34\n",
        "progress_bar_max_val = total_number_of_tile_types * \\\n",
        "                        target_number_of_annotations_for_each_tile_types\n",
        "\n",
        "bar = progressbar.ProgressBar(max_value=progress_bar_max_val, \n",
        "                              min_value=0, \n",
        "                              widgets=[progressbar.Bar('=', '[', ']'),\n",
        "                                       ' ', \n",
        "                                       progressbar.Percentage()])\n",
        "annotations_number_for_updaing_progress_bar = 0\n",
        "bar.start()\n",
        "bar.update(annotations_number_for_updaing_progress_bar)\n",
        "\n",
        "# Generate images until the number of annotation target is met\n",
        "annotation_target_is_met = False\n",
        "total_number_of_images_generated = 0\n",
        "while not annotation_target_is_met:  \n",
        "  grid_length = random.randint(min_grid_length, max_grid_length)\n",
        "  tile_types_annotated = create_image_in_grid_format(csv_filename, \n",
        "                                                     grid_length)\n",
        "  total_number_of_images_generated += 1\n",
        "  for tile_type in tile_types_annotated:\n",
        "    tile_type_annotations[tile_type] += 1\n",
        "  annotations_number_for_updaing_progress_bar = \\\n",
        "  count_annotations_number_for_updating_progress_bar(\n",
        "      tile_type_annotations, \n",
        "      target_number_of_annotations_for_each_tile_types)\n",
        "  bar.update(annotations_number_for_updaing_progress_bar)\n",
        "  annotation_target_is_met = \\\n",
        "  check_if_annotation_target_is_met(tile_type_annotations, target_number_of_annotations_for_each_tile_types)\n",
        "\n",
        "bar.finish()\n",
        "print(\"Total number of images generated: \" + str(total_number_of_images_generated))\n",
        "\n",
        "total_number_of_annotations = sum(tile_type_annotations.values())\n",
        "print(\"Total number of annotations created: \" + str(total_number_of_annotations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Auto ML to Yolo Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "# from collections import defaultdict\n",
        "\n",
        "# Define file paths\n",
        "csv_file_path = 'dataset-for-training-AutoML-Vision-model.csv'\n",
        "output_directory = 'training/yolo_format/'\n",
        "class_mapping_file_path = 'yolo_class_mapping.txt'\n",
        "\n",
        "# Make sure the output directory exists\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Initialize a set to hold unique class names\n",
        "class_names = set()\n",
        "with open(csv_file_path, newline='') as csvfile:\n",
        "    reader = csv.reader((line.replace(',,', ', ,') for line in csvfile))  # Replace 'empty' fields marked by ',,' with ', ,'\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        class_names.add(row[2])  # Add class label to the set\n",
        "\n",
        "# Convert class names set to a list and sort it for consistent class index mapping\n",
        "class_names = sorted(list(class_names))\n",
        "\n",
        "# Create a dictionary to map class names to indices\n",
        "class_mapping = {name: index for index, name in enumerate(class_names)}\n",
        "\n",
        "# Process the CSV file to convert to YOLO format\n",
        "with open(csv_file_path, newline='') as csvfile:\n",
        "    reader = csv.reader((line.replace(',,', ', ,') for line in csvfile))  # Replace 'empty' fields marked by ',,' with ', ,'\n",
        "    next(reader)  # Skip the header\n",
        "    \n",
        "    for row in reader:\n",
        "        set_type = row[0].lower()\n",
        "        img_file = os.path.basename(row[1])\n",
        "        class_label = row[2]\n",
        "        x_min = float(row[3])\n",
        "        y_min = float(row[4])\n",
        "        x_max = float(row[7])\n",
        "        y_max = float(row[8])\n",
        "\n",
        "        x_center = (x_min + x_max) / 2\n",
        "        y_center = (y_min + y_max) / 2\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        class_index = class_mapping[class_label]\n",
        "\n",
        "        txt_file_path = os.path.join(output_directory, f\"{set_type}_labels\", img_file.replace('.jpg', '.txt'))\n",
        "        os.makedirs(os.path.dirname(txt_file_path), exist_ok=True)\n",
        "        \n",
        "        with open(txt_file_path, 'a') as yolo_file:\n",
        "            yolo_file.write(f\"{class_index} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "# Save the class mappings to a text file so you have a reference to which indices correspond to which classes\n",
        "\n",
        "with open(os.path.join(output_directory, class_mapping_file_path), 'w') as class_file:\n",
        "    for class_name, class_index in class_mapping.items():\n",
        "        class_file.write(f\"{class_index}:{class_name}\\n\")\n",
        "        \n",
        "with open(os.path.join(output_directory,\"classes.names\"), 'w') as class_file:\n",
        "    for class_name, class_index in class_mapping.items():\n",
        "        class_file.write(f\"{class_name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bamboo-1', 'bamboo-2', 'bamboo-3', 'bamboo-4', 'bamboo-5', 'bamboo-6', 'bamboo-7', 'bamboo-8', 'bamboo-9', 'characters-1', 'characters-2', 'characters-3', 'characters-4', 'characters-5', 'characters-6', 'characters-7', 'characters-8', 'characters-9', 'dots-1', 'dots-2', 'dots-3', 'dots-4', 'dots-5', 'dots-6', 'dots-7', 'dots-8', 'dots-9', 'honors-east', 'honors-green', 'honors-north', 'honors-red', 'honors-south', 'honors-west', 'honors-white']\n"
          ]
        }
      ],
      "source": [
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clone yolo and start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone https://github.com/ultralytics/yolov5.git\n",
        "    cd yolov5\n",
        "    pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying train: \u001b[38;2;255;0;0m  0%\u001b[39m |/                                         | ETA:  --:--:--\n",
            "Copying train: \u001b[38;2;255;104;0m 17%\u001b[39m |-                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;255;154;0m 35%\u001b[39m |\\                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;255;239;0m 54%\u001b[39m ||                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;212;255;0m 72%\u001b[39m |/                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;159;255;0m 91%\u001b[39m |-                                         | ETA:   0:00:00\n",
            "Copying train: \u001b[38;2;0;255;0m100%\u001b[39m ||                                         | Time:  0:00:00\n",
            "Copying val: \u001b[38;2;255;0;0m  0%\u001b[39m |/                                           | ETA:  --:--:--\n",
            "Copying val: \u001b[38;2;0;255;0m100%\u001b[39m ||                                           | Time:  0:00:00\n",
            "Copying test: \u001b[38;2;255;0;0m  0%\u001b[39m |/                                          | ETA:  --:--:--\n",
            "Copying test: \u001b[38;2;0;255;0m100%\u001b[39m ||                                          | Time:  0:00:00\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define your current directories\n",
        "labels_directory = 'training/yolo_format'\n",
        "images_directory = 'images'  # Assuming this contains all images (unsorted)\n",
        "\n",
        "# Define the base directory for the YOLOv5 project\n",
        "yolov5_base_directory = './yolov5'\n",
        "dataset_name = 'mahjong'  # Name of your dataset\n",
        "\n",
        "# Define the new paths for images and labels\n",
        "new_dataset_base = os.path.join(yolov5_base_directory, 'datasets', dataset_name)\n",
        "new_images_base = os.path.join(new_dataset_base, 'images')\n",
        "new_labels_base = os.path.join(new_dataset_base, 'labels')\n",
        "\n",
        "# Set names for train, val, and test sets\n",
        "sets = ['train', 'val', 'test']\n",
        "\n",
        "# Create the directories for images and labels based on sets if they do not exist\n",
        "for set_name in sets:\n",
        "    os.makedirs(os.path.join(new_images_base, set_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(new_labels_base, set_name), exist_ok=True)\n",
        "\n",
        "# Function to copy labels and images with a progress bar\n",
        "def copy_labels_and_images_with_progress(label_set_name, src_labels_dir, dest_images_dir, dest_labels_dir):\n",
        "    if not os.path.exists(src_labels_dir):\n",
        "        print(f\"No labels directory found for {label_set_name}, skipping.\")\n",
        "        return\n",
        "\n",
        "    label_files = os.listdir(src_labels_dir)\n",
        "    widgets = [\n",
        "        f'Copying {label_set_name}: ', progressbar.Percentage(), ' ',\n",
        "        progressbar.Bar(marker=progressbar.RotatingMarker()), ' ', progressbar.ETA()\n",
        "    ]\n",
        "    bar = progressbar.ProgressBar(widgets=widgets, max_value=len(label_files)).start()\n",
        "\n",
        "    for i, label_file in enumerate(label_files):\n",
        "        # Copy label file\n",
        "        src_label_path = os.path.join(src_labels_dir, label_file)\n",
        "        dest_label_path = os.path.join(dest_labels_dir, label_file)\n",
        "        shutil.copy(src_label_path, dest_label_path)\n",
        "\n",
        "        # Determine corresponding image file and copy it\n",
        "        image_file_name = label_file.replace('.txt', '.jpg')  # Assumes images are .jpg\n",
        "        src_image_path = os.path.join(images_directory, image_file_name)\n",
        "        dest_image_path = os.path.join(dest_images_dir, image_file_name)\n",
        "        if os.path.exists(src_image_path):\n",
        "            shutil.copy(src_image_path, dest_image_path)\n",
        "        else:\n",
        "            print(f\"Warning: Corresponding image file not found for {src_image_path}\")\n",
        "\n",
        "        bar.update(i+1)\n",
        "    bar.finish()\n",
        "\n",
        "# Copy labels and images for each set using the progress bar\n",
        "for set_name in sets:\n",
        "    copy_labels_and_images_with_progress(\n",
        "        label_set_name=set_name,\n",
        "        src_labels_dir=os.path.join(labels_directory, f\"{set_name}_labels\"),\n",
        "        dest_images_dir=os.path.join(new_images_base, set_name),\n",
        "        dest_labels_dir=os.path.join(new_labels_base, set_name),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cp mahjong.yaml yolov5/\n",
        "!cp class_names.yaml yolov5/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=./models/yolov5s.yaml, data=mahjong.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=1024, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=mps, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=mahjong_train, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v7.0-294-gdb125a20 Python-3.12.1 torch-2.2.1 MPS\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=34\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    105183  models.yolo.Detect                      [34, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7111327 parameters, 7111327 gradients, 16.2 GFLOPs\n",
            "\n",
            "Transferred 342/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/yolov5/datasets/mahjong/labels/train.cache... 801 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/yolov5/datasets/mahjong/labels/val.cache... 89 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.54 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Plotting labels to runs/train/mahjong_train2/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/mahjong_train2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "  0%|          | 0/51 [00:00<?, ?it/s]/AppleInternal/Library/BuildRoots/ce725a5f-c761-11ee-a4ec-b6ef2fd8d87b/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShaders/MPSCore/Types/MPSNDArray.mm:124: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (3) is not less than length of dimension[1] (3)'\n",
            "bash: line 2: 44644 Abort trap: 6           python train.py --device mps --img 1024 --batch 16 --epochs 100 --data mahjong.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name mahjong_train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cd yolov5/\n",
        "python train.py --include coreml --img 1024 --batch 16 --epochs 100 --data mahjong.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name mahjong_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.12.1 torch-2.2.1 CPU (Apple M1 Max)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=mahjong_yolov8.yaml, epochs=100, time=None, patience=100, batch=32, imgsz=1024, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/xingfanxia/projects/mahjong_hand_classifier/runs/detect/train\n",
            "Overriding model.yaml nc=80 with nc=34\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3795382  ultralytics.nn.modules.head.Detect           [34, [192, 384, 576]]         \n",
            "Model summary: 295 layers, 25876006 parameters, 25875990 gradients, 79.2 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/xingfanxia/mahjong-yolov8-cpu/fa2dc3036e2b45b8873fcd12b84e9743\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/datasets/mahjong/labels/train.cache... 801 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 801/801 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/xingfanxia/projects/mahjong_hand_classifier/datasets/mahjong/labels/val.cache... 89 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to /Users/xingfanxia/projects/mahjong_hand_classifier/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000263, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/Users/xingfanxia/projects/mahjong_hand_classifier/runs/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      1/100         0G     0.3197      3.198     0.9211         53       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [54:25<00:00, 125.60s/it] \n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:17<00:00, 68.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         89       2394      0.222      0.469      0.229      0.228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      2/100         0G     0.2331       1.36     0.8445         43       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [51:48<00:00, 119.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:11<00:00, 65.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         89       2394      0.696      0.783      0.801      0.798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      3/100         0G     0.1991     0.7078     0.8255         22       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [51:59<00:00, 120.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:11<00:00, 65.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         89       2394      0.921      0.945      0.973      0.972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      4/100         0G     0.1989     0.5141     0.8217         41       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [52:07<00:00, 120.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:11<00:00, 65.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         89       2394      0.968      0.963       0.99      0.989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      5/100         0G     0.1903     0.4594     0.8207         17       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [52:04<00:00, 120.19s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:11<00:00, 65.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         89       2394      0.986      0.974      0.993      0.992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      6/100         0G     0.1854     0.3879      0.812         86       1024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [52:02<00:00, 120.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:12<00:00, 66.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         89       2394      0.987      0.991      0.994      0.993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      7/100         0G     0.1671     0.3532     0.8096       1503       1024:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 23/26 [49:49<06:34, 131.36s/it]"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8m.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "model.train(data=\"mahjong_yolov8.yaml\", imgsz=1024, epochs=100, batch=32)  # train the model\n",
        "metrics = model.val()  # evaluate model performance on the validation set\n",
        "# results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "onnx_path = model.export()  # export the model to ONNX format\n",
        "coreml_path = model.export(format=\"coreml\")  # export the model to CoreML format\n",
        "tflite_path = model.export(format=\"tflite\")  # export the model to TensorFlow Lite format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mahjong-dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
